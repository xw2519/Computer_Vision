{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 2: Fish Classification\n",
    "\n",
    "Created by Athanasios Vlontzos and Wenjia Bai\n",
    "\n",
    "In this coursework, you will be exploring the application of convolutional neural networks for image classification tasks. As opposed to standard applications such as object or face classification, we will be dealing with a slightly different domain, fish classification for precision fishing.\n",
    "\n",
    "In precision fishing, engineers and fishmen collaborate to extract a wide variety of information about the fish, their species and wellbeing etc. using data from satellite images to drones surveying the fisheries. The goal of precision fishing is to provide the marine industry with information to support their decision making processes.\n",
    "\n",
    "Here your will develop an image classification model that can classify fish species given input images. It consists of two tasks. The first task is to train a model for the following species:\n",
    "- Black Sea Sprat\n",
    "- Gilt-Head Bream\n",
    "- Shrimp\n",
    "- Striped Red Mullet\n",
    "- Trout\n",
    "\n",
    "The second task is to finetune the last layer of the trained model to adapt to some new species, including:\n",
    "- Hourse Mackerel\n",
    "- Red Mullet\n",
    "- Red Sea Bream\n",
    "- Sea Bass\n",
    "\n",
    "You will be working using a large-scale fish dataset [1].\n",
    "\n",
    "[1] O. Ulucan, D. Karakaya and M. Turkan. A large-scale dataset for fish segmentation and classification. Innovations in Intelligent Systems and Applications Conference (ASYU). 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Download data.\n",
    "\n",
    "[Download the Data from here -- make sure you access it with your Imperial account.](https://imperiallondon-my.sharepoint.com/:f:/g/personal/av2514_ic_ac_uk/EkA9HyXVvgdFoLI4P_IfO1cBO_CsvY1KN4NE8iuD-s_VlA?e=Ip03rF)\n",
    "\n",
    "It is a ~2.5GB file. You can save the images and annotations directories in the same directory as this notebook or somewhere else.\n",
    "\n",
    "The fish dataset contains 9 species of fishes. There are 1,000 images for each fish species, named as %05d.png in each subdirectory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the data. (15 Points)\n",
    "\n",
    "- Complete the dataset class with the skeleton below.\n",
    "- Add any transforms you feel are necessary.\n",
    "\n",
    "Your class should have at least 3 elements\n",
    "- An ```__init__``` function that sets up your class and all the necessary parameters.\n",
    "- An ```__len__``` function that returns the size of your dataset.\n",
    "- An ```__getitem__``` function that given an index within the limits of the size of the dataset returns the associated image and label in tensor form.\n",
    "\n",
    "You may add more helper functions if you want.\n",
    "\n",
    "In this section we are following the Pytorch [dataset](https://pytorch.org/vision/stable/datasets.html) class structure. You can take inspiration from their documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, datasets\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We will start by building a dataset class using the following 5 species of fishes\n",
    "Multiclass_labels_correspondances = {\n",
    "    'Black Sea Sprat': 0,\n",
    "    'Gilt-Head Bream': 1,\n",
    "    'Shrimp': 2,\n",
    "    'Striped Red Mullet': 3,\n",
    "    'Trout': 4\n",
    "}\n",
    "\n",
    "# The 5 species will contain 5,000 images in total.\n",
    "# Let us split the 5,000 images into training (80%) and test (20%) sets\n",
    "def split_train_test(lendata, percentage=0.8):\n",
    "    idxs_train = int(lendata*percentage)\n",
    "    idxs_test = int(lendata - idxs_train)\n",
    "    \n",
    "    return idxs_train, idxs_test\n",
    "\n",
    "LENDATA = 5000\n",
    "np.random.seed(42)\n",
    "idxs_train, idxs_test = split_train_test(LENDATA,0.8)\n",
    "\n",
    "# Seaborn style \n",
    "%matplotlib inline \n",
    "seaborn.set_style('darkgrid')\n",
    "\n",
    "# Implement the dataset class\n",
    "class FishDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 path_to_images,\n",
    "                 idxs_train,\n",
    "                 idxs_test,\n",
    "                 transform_extra=None,\n",
    "                 img_size=128,\n",
    "                 train=True):\n",
    "        # path_to_images: where you put the fish dataset\n",
    "        # idxs_train: training set indexes\n",
    "        # idxs_test: test set indexes\n",
    "        # transform_extra: extra data transform\n",
    "        # img_size: resize all images to a standard size\n",
    "        # train: return training set or test set\n",
    "        \n",
    "        # Load all the images and their labels\n",
    "        # Resize all images to a standard size\n",
    "        transform_extra = transforms.Resize(size=(img_size, img_size))\n",
    "        self.dataset = datasets.ImageFolder(path_to_images, transform=transform_extra)\n",
    "        self.len = len(self.dataset.imgs)\n",
    "    \n",
    "        # Extract the images and labels with the specified file indexes    \n",
    "        self.train_subset, self.test_subset = random_split(self.dataset, [idxs_train, idxs_test])\n",
    "        \n",
    "        self.train_dataset = DataLoader(self.train_subset, shuffle=True)\n",
    "        self.test_dataset = DataLoader(self.test_subset, shuffle=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Return the number of samples\n",
    "        return self.len\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Get an item using its index\n",
    "        # Return the image and its label\n",
    "        return self.dataset.__getitem__(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Explore the data. (15 Points)\n",
    "\n",
    "### Step 2.1: Data visualisation. (5 points)\n",
    "\n",
    "- Plot data distribution, i.e. the number of samples per class.\n",
    "- Plot 1 sample from each of the five classes in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19792/2285528190.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mseaborn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mget_class_distribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmelt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"variable\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"variable\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdodge\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Number of samples per class'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbbox_to_anchor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.02\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'upper left'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mborderaxespad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19792/2285528190.py\u001b[0m in \u001b[0;36mget_class_distribution\u001b[1;34m(dataset_object)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_class_distribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mcount_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_to_idx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset_object\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attribute_name)\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training set\n",
    "img_path = './Task_1_Dataset'\n",
    "dataset = FishDataset(img_path, idxs_train, idxs_test, None, img_size=128, train=True)\n",
    "\n",
    "idx2class = {v: k for k, v in dataset.dataset.class_to_idx.items()}\n",
    "\n",
    "def get_class_distribution(dataset_object):\n",
    "    count_dict = {k:0 for k,v in dataset.dataset.class_to_idx.items()}\n",
    "\n",
    "    for element in dataset_object:\n",
    "        y_lbl = element[1]\n",
    "        y_lbl = idx2class[y_lbl]\n",
    "        count_dict[y_lbl] += 1\n",
    "        \n",
    "    return count_dict\n",
    "\n",
    "# Plot the number of samples per class\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "seaborn.barplot(data=pd.DataFrame.from_dict([get_class_distribution(dataset.train_subset)]).melt(), x = \"variable\", y = \"value\", hue = \"variable\", dodge=False).set_title('Number of samples per class')\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "\n",
    "# Plot 1 sample from each of the five classes in the training set\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 2\n",
    "\n",
    "classes_seen = set()\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    # Iterate until a class not displayed is fetch \n",
    "    sample_idx = torch.randint(len(dataset.train_subset), size=(1,)).item()\n",
    "    img, label = dataset.train_subset[sample_idx]\n",
    "    \n",
    "    if label not in classes_seen:\n",
    "        classes_seen.add(label)\n",
    "        figure.add_subplot(rows, cols, i)\n",
    "        plt.title(idx2class[label])\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(img, cmap=\"gray\")\n",
    "        i += 1\n",
    "    \n",
    "    if i == 6: break\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Discussion. (10 points)\n",
    "\n",
    "* Is the dataset balanced?\n",
    "\n",
    "* Can you think of 3 ways to make the dataset balanced if it is not?\n",
    "\n",
    "* Is the dataset already pre-processed? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADD YOUR RESPONSE HERE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Multiclass classification. (55 points)\n",
    "In this section we will try to make a multiclass classifier to determine the species of the fish.\n",
    "\n",
    "### Step 3.1: Define the model. (15 points)\n",
    "\n",
    "Design a neural network which consists of a number of convolutional layers and a few fully connected ones at the end.\n",
    "\n",
    "The exact architecture is up to you but you do NOT need to create something complicated. For example, you could design a LeNet insprired network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, output_dims = 1):\n",
    "        super(Net, self).__init__()\n",
    "        #### ADD YOUR CODE HERE ####\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward propagation\n",
    "        #### ADD YOUR CODE HERE ####\n",
    "\n",
    "        return x\n",
    "\n",
    "# Since most of you use laptops, you may use CPU for training.\n",
    "# If you have a good GPU, you can set this to 'gpu'.\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2: Define the training parameters. (10 points)\n",
    "\n",
    "- Loss function\n",
    "- Optimizer\n",
    "- Learning Rate\n",
    "- Number of iterations\n",
    "- Batch Size\n",
    "- Other relevant hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Network\n",
    "model =\n",
    "\n",
    "# Loss function\n",
    "criterion =\n",
    "\n",
    "# Optimiser and learning rate\n",
    "lr =\n",
    "optimizer =\n",
    "\n",
    "# Number of iterations for training\n",
    "epochs =\n",
    "\n",
    "# Training batch size\n",
    "train_batch_size =\n",
    "\n",
    "# Based on the FishDataset, use the PyTorch DataLoader to load the data during model training\n",
    "train_dataset =\n",
    "train_dataloader =\n",
    "test_dataset =\n",
    "test_dataloader ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.3: Train the model. (15 points)\n",
    "\n",
    "Complete the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    loss_curve = []\n",
    "    \n",
    "    for imgs, labs in train_dataloader:\n",
    "        # Get a batch of training data and train the model\n",
    "        #### ADD YOUR CODE HERE ####\n",
    "        \n",
    "        loss_curve += [loss.item()]\n",
    "    print('--- Iteration {0}: training loss = {1:.4f} ---'.format(epoch + 1, np.array(loss_curve).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.4: Deploy the trained model onto the test set. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Deploy the model\n",
    "#### ADD YOUR CODE HERE ####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.5: Evaluate the performance of the model and visualize the confusion matrix. (5 points)\n",
    "\n",
    "You can use sklearns related function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#### ADD YOUR CODE HERE ####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Finetune your classifier. (15 points)\n",
    "\n",
    "In the previous section, you have built a pretty good classifier for certain species of fish. Now we are going to use this trained classifier and adapt it to classify a new set of species:\n",
    "\n",
    "    'Hourse Mackerel\n",
    "    'Red Mullet',\n",
    "    'Red Sea Bream'\n",
    "    'Sea Bass'\n",
    "\n",
    "### Step 4.1: Set up the data for new species. (2 points)\n",
    "Overwrite the labels correspondances so they only incude the new classes and regenerate the datasets and dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Multiclass_labels_correspondances ={\n",
    "    'Hourse Mackerel': 0,\n",
    "    'Red Mullet': 1,\n",
    "    'Red Sea Bream': 2,\n",
    "    'Sea Bass': 3}\n",
    "\n",
    "LENDATA = 4000\n",
    "idxs_train,idxs_test = split_train_test(LENDATA, 0.8)\n",
    "\n",
    "# Dataloaders\n",
    "#### ADD YOUR CODE HERE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2: Freeze the weights of all previous layers of the network except the last layer. (5 points)\n",
    "\n",
    "You can freeze them by setting the gradient requirements to ```False```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def freeze_till_last(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "freeze_till_last(model)\n",
    "# Modify the last layer. This layer is not freezed.\n",
    "#### ADD YOUR CODE HERE ####\n",
    "\n",
    "# Loss function\n",
    "criterion =\n",
    "\n",
    "# Optimiser and learning rate\n",
    "lr =\n",
    "optimizer =\n",
    "\n",
    "# Number of iterations for training\n",
    "epochs =\n",
    "\n",
    "# Training batch size\n",
    "train_batch_size ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.3: Train and test your finetuned model. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Finetune the model\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    #### ADD YOUR CODE HERE ####\n",
    "\n",
    "# Deploy the model on the test set\n",
    "#### ADD YOUR CODE HERE ####\n",
    "\n",
    "# Evaluate the performance\n",
    "#### ADD YOUR CODE HERE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.4: Did finetuning work? Why did we freeze the first few layers? (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADD YOUR RESPONSE HERE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
