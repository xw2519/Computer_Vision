\documentclass{report}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, total={7in, 10in}]{geometry}
\usepackage{graphicx}
\usepackage[parfill]{parskip}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{subfig}
\usepackage{titlesec}

\titleformat{\chapter}[display]
{\normalfont\huge\bfseries}{\chaptertitlename\ \thechapter}{20pt}{\Huge}
\titlespacing*{\chapter}{0pt}{0pt}{40pt}


\graphicspath{ {./Images/} }
\setlength{\parindent}{0em}

\title{Computer Vision} 
\author{Xin Wang}

\begin{document}
\par
\maketitle
\medskip

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Image Formation}

A digital image is formed from three components:
\begin{enumerate}
    \item Lighting 
    \item Reflectance 
    \item Optics and Sensors
\end{enumerate} 

\section{Lighting}

Lighting has several properties of concern:
\begin{itemize}
    \item Location 
    \item Intensity of light 
    \item Spectrum of light
\end{itemize} 

There are many different sources of light. Four commonly used models to describe
light sources are:
\begin{itemize}
    \item \textbf{Point Light Source}: The light is inside the scene at a
    specific location only and it shines light equally in all directions. An example
    is be a table lamp.
    \item \textbf{Area Light Source}: The light source comes from a rectangular
    area and projects light from one side of the rectangle. An example is a
    florescent light fixture in a ceiling panel. 
    \item \textbf{Sun Light Source}: The light is outside the scene and far
    enough away that all rays of light are basically from the same direction. 
    An example is the sun in an outdoor scene.
    \item \textbf{Spotlight Light Source}: The light is focused and forms a
    cone-shaped envelop as it projects out from the light source. An example is
    a spotlight in a theatre. 
\end{itemize}

\section{Reflectance}

\begin{wrapfigure}[10]{r}{0.45\linewidth}
\centering
\includegraphics[width=6cm]{BRDF.JPG}
\caption{BRDF model}
\label{fig:myfig}
\end{wrapfigure}

A general model for modelling reflectance is the
\textbf{Bidirectional Reflectance Distribution Function (BRDF)}. The model
describes how much light arriving at incident direction is emitted in
reflected direction. 
$$
f_r\left(\theta_i, \sigma_i, \theta_r, \sigma_r, \lambda\right) = \frac{dL_r}{dE_i}
$$
where:
\begin{itemize}
    \item $\theta_i$ and $\sigma_i$: Incident direction
    \item $\theta_r$ and $\sigma_r$: Reflected direction
    \item $\lambda$: Wavelength
    \item $dL_r$: Output power 
    \item $dE_i$: Input power 
\end{itemize}

\pagebreak

Considering the two ideal cases of reflection: \textbf{Diffuse reflection} and
\textbf{Specular reflection}
\begin{itemize}
    \item \textbf{Diffuse reflection}: 
    \begin{figure}[h!]
        \centering
        \includegraphics[width=.2\linewidth]{Diffuse reflection angle.JPG}
        \qquad
        \includegraphics[width=.1\linewidth]{Diffuse reflection view.JPG}
    \end{figure}

    The light is scattered uniformly in all directions so the BRDF is constant:
    $$
        f_r\left(\theta_i, \sigma_i, \theta_r, \sigma_r, \lambda\right) = f_r(\lambda)
    $$

    This effect on the viewed image is similar to the appearance of a rough
    surface.
    
    \item \textbf{Specular reflection}: 
    \begin{figure}[h!]
        \centering
        \includegraphics[width=.15\linewidth]{Specular reflection angle.JPG}
        \qquad
        \includegraphics[width=.1\linewidth]{Specular reflection view.JPG}
    \end{figure}

    The light is reflected in a mirror-like fashion. The reflection and incident directions are symmetric with respect to the
    surface normal $\vec{n}$: $\theta_r = \theta_i$
\end{itemize}

For the majority of cases in the real world, there is a combination of
\textbf{diffuse reflection}, \textbf{specular reflection} and \textbf{ambient
illumination}. Ambient illumination accounts for general illumination which may be
complicated to model such as the inter-reflection between walls in a room and
distant light sources as seen in sunny outdoor environments.

This observation is formally state in the \textbf{Phong Reflection Model} which
is an empirical model in computer graphics that describes how a surface reflects
light as a combination of ambient, diffuse and specular components. 
\begin{figure}[h]
    \centering
    \includegraphics[width=10cm]{Phong reflection model.JPG}
\end{figure}

\subsection{Computer graphics}

There is an unique relationship between computer graphics and computer vision
similar to the relationship of writing and reading. Computer graphics is
concerned with generating images while computer vision is concerned with
intepreting images. 

This relationship is particularly valuable as computer graphics in
photorealistic games can be used to generate data i.e. images + labels for
training computer vision algorithms. 

\section{Optics}

Camera sensors imitates the human eye which are human sensors. In the human eye,
there are two types of neural cells in the retina: 
\begin{itemize}
    \item \textbf{Cone cells}: Colour vision and functions in the bright light. 
    \item \textbf{Rod cells}: More sensitive to the light but monochromatic and
    functions in the dim light like night time. 
\end{itemize}

Camera sensors are very much the same: \textbf{Charge-coupled Device (CCD)} and
\textbf{Complementary Metal-oxide Semiconductor (CMOS)}.

\subsection{Colour Filter Arrays (CFA)}

A color filter array (CFA) or color filter mosaic (CFM) is a mosaic of tiny
color filters placed over the pixel sensors of an image sensor to capture color
information.  

The most common CFA is the most single-chip digital image sensors used in
digital cameras to create a colour image is the \textit{Bayer Filter Mosaic}.
The Bayer Filter Mosaic is arraged to mimic the human eyes i.e. most sensitive
to green light with $50\%$ green, $25\%$ red and $25\%$ blue.

The RGB of different cameras may be different, i.e. with different sensitivities
to wavelengths. This different colour sensitivity is why the same picture with
different cameras would look different.

\subsection{Bayer Colour Filter}

With its arrangement, only one colour is available at each pixel. The other two
colours can be interpolated from neighbouring pixels. Through this
interpolation at each pixel, the RGB values can be obtained. 
\begin{figure}[h]
    \centering
    \includegraphics[width=10cm]{Bayer colour filter.JPG}
    \caption{Bayer colour filter and interpolation}
\end{figure}

\subsection{Demosaicing}

A demosaicing algorithm is a digital image process used to reconstruct a full
colour image from incomplete colour samples output from an image sensor overlaid
with a CFA.

A simple method is bilinear interpolation where the red value of a non-red pixel
is computed as the average of the two or four adjacent red pixels, and similarly
for blue and green.  
\begin{figure}[h]
    \centering
    \includegraphics[width=10cm]{Demosaicing algorithm.JPG}
\end{figure}

\subsection{Colour spaces}

\begin{wrapfigure}[10]{r}{0.35\linewidth}
    \centering
    \includegraphics[width=5cm]{Colour space repre.JPG}
    \caption{CIE 1931}
\end{wrapfigure}

Colour spaces are mathematical models describing the way colours can be
represented. Colour spaces can be seen as a box containing all possible colours that can be
produced by mixing primary colours of RGB. 

The CIE 1931 XYZ was created by the International Commission on Illumination and
represents the colour space using the primary colours X, Y and Z. 
\begin{align*}
    x &= \frac{X}{X+Y+Z} \\
    y &= \frac{Y}{X+Y+Z} \\
    z &= \frac{Z}{X+Y+Z}
\end{align*}

There are various different forms colour spaces such as sRGB, HSV and CMYK.  All
these colour spaces can represent the same colour, but using different
primary colours or coordinate systems.

\subsection{Quantisation}

Quantisation is the process that maps continuous signal to discrete signal. For
colours, color quantization reduces the number of colors used in an image. This
is important for displaying images on devices that support a limited number of
colors and for efficiently compressing certain kinds of images.

It is important to note that numerical errors occur during quantisation,
which depends on the number of bits used. The more bits, the less quantisation
error.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Image Filtering}

The goal of using filters is to modify or enhance image properties and/or to
extract valuable information from the pictures such as edges, corners, and
blobs.

There are various types of image filters such as:
\begin{itemize}
    \item Identity filter
    \item Low-pass/Smoothing filters: Moving average filter or Gaussian filter 
    \item High-pass/Sharpening filters 
    \item Denoising filter: Median filter
\end{itemize}

\section{Smoothing Filters: Moving Average Filter}

A moving average filter moves a window across the signal and calculates the
average value within the window. A \textbf{filter kernal} is specified to
indicate what values are averaged in the moving average filter. A filter
kernal \footnote{https://en.wikipedia.org/wiki/Kernel\_(image\_processing)}
is a small matrix used for blurring, sharpening, edge detection etc. 

In images, the moving average filter removes high frequency signal e.g. noise or
sharpness. This operation results in a smooth but blurry image. For example,
consider a \textbf{box blur} kernal and its effect:

\begin{minipage}{7in}
    \centering
    \raisebox{-0.5\height}{\includegraphics[height=0.5in]{Box blur.JPG}}
    \hspace*{.2in}
    \raisebox{-0.5\height}{\includegraphics[height=1.5in]{Box blur calculation.JPG}}
\end{minipage}

Due to the nature of this method of calculations, the output image will be
smaller than the input image. The boundary pixels are generally dealt with using
padding of various methods such as constant value, mirroring values etc. The
following example uses 0 padding for its boundary pixels. 

\begin{minipage}{7in}
    \centering
    \raisebox{-0.5\height}{\includegraphics[height=1.5in]{Zero padding.JPG}}
    \hspace*{.2in}
    \raisebox{-0.5\height}{\includegraphics[height=1.5in]{Blur images.JPG}}
\end{minipage}

By increasing the size of the kernal e.g. into a $7 \times 7$ matrix, the image
will become blurrier. 

\vfill

\subsection{Brute Computational Complexity}

Note that: \textit{Image size}: $N \times N$ where $N$ is the number of pixels and
\textit{Kernal size}: $K \times K$ where $K$ is size of the filter kernal matrix 
\begin{itemize}
    \item There are $N^2$ pixels 
    \item At each pixel, there are $K^2$ multiplications and $K^2 - 1$ summations.
    \item In total, there are: $N^2 K^2$ multiplications and $N^2(K^2 - 1)$
    summations 
    \item Complexity is: $O(N^2 K^2)$
\end{itemize}

\subsection{Separable filter}

If a big filter can be separated as the consecutive operation of two small
filters, then the first filter can be applied to the the input image then the
second filter. For example, consider the previous blur filter kernal divided
into two smaller filters:
\begin{figure}[h]
    \centering
    \includegraphics[width=6cm]{Blur separable filter.JPG}
\end{figure}

This calculation procedure results in the same result as the previous result:

\begin{minipage}{7in}
    \centering
    \raisebox{-0.5\height}{\includegraphics[height=1.5in]{X separable filter 1.JPG}}
    \hspace*{.2in}
    \raisebox{-0.5\height}{\includegraphics[height=1.5in]{X separable filter 2.JPG}}
\end{minipage}

\begin{minipage}{7in}
    \centering
    \raisebox{-0.5\height}{\includegraphics[height=1.5in]{Y separable filter 1.JPG}}
    \hspace*{.2in}
    \raisebox{-0.5\height}{\includegraphics[height=1.5in]{Y separable filter 2.JPG}}
\end{minipage}

\subsection{Separable filter complexity}

Note that: \textit{Image size}: $N \times N$ where $N$ is the number of pixels
and there are two filter kernals: $1 \times K$ and $K \times 1$.
\begin{itemize}
    \item There are $N^2$ pixels 
    \item At each pixel, there are $K$ multiplications and $K - 1$ summations.
    \item In total, there are: $2N^2 K$ multiplications and $2N^2(K - 1)$
    summations 
    \item Complexity is: $O(N^2 K)$ which is faster than the previous $O(N^2 K^2)$
\end{itemize}

\section{Identity Filter}

The \textbf{Identity Filter Kernal} simply returns the same value of the image
i.e. the input and output image is the same.

\section{Smoothing Filters: Gaussian Filter}

The Gaussian Filter uses a 2D Gaussian Distribution as its filter kernal:
$$
    h(i,j) = \frac{1}{2\pi \sigma^2} e^{- \frac{i^2 + j^2}{2\sigma^2}}
$$

The 2D Gaussian filter is a separable filter, equivalent to two 1D Gaussian
filters with the same $\sigma$, one along x-axis and the other along y-axis:
\begin{align*}
    h(i,j) &= h_x(i) * h_y(j) \\
    h_x(i) &= \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{i^2}{2\sigma^2}}
\end{align*}

\section{High-pass Filters}

There are various methods of designing high-pass filters including using
low-pass filters as seen in Design 1.
\begin{figure}[h]
    \centering
    \includegraphics[width=10cm]{High pass.JPG}
\end{figure}

\section{Denoising Filters: Median Filter}

Median filters are non-linear filters that is often used for denoising an image.
A common method of performing median filter is to move the sliding window and
replacing the centre pixel using the \textit{median value} in the window.
\begin{figure}[h]
    \centering
    \includegraphics[width=10cm]{Median filter.JPG}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Image Filtering II}

The concept of filtering can be described mathematically for example: 
\begin{figure}[h]
    \centering
    \includegraphics[width=8cm]{Filter_mathematical_example 1.JPG}
\end{figure}
where this operations can be described as:
$$
    g[n] = \frac{1}{3} . f[n-1] + \frac{1}{3} . f[n] + \frac{1}{3} . f[n+1]
$$

This concept of describing filters mathematically is extensively used in Signal
Processing where it describes filters as:
\begin{center}
    \textit{A device/process '$h$' that removes unwanted components/features from a input
    signal '$f$' and generates an output signal '$g$'}
\end{center}

\section*{Impulse Response}

Impulse responses are used to mathematically describe a filter. The impulse
response $h$ is the output of a filter when the input is an impulse signal $\delta$. 
\begin{figure}[h]
    \centering
    \includegraphics[width=5cm]{Impulse response.JPG}
\end{figure}

\begin{itemize}
    \item \textbf{Continuous signal}: An impulse signal is the \textit{Dirac
    Delta function} $\delta(x)$:
    \begin{gather*}
        \delta(x) = \begin{cases}
            +\infty & \text{if } x = 0 \\
            0 & \text{otherwise}
        \end{cases} \\
        \int^{\infty}\delta (x) dx = 1
    \end{gather*}

    \item \textbf{Discrete signal}: An impulse signal is the \textit{Kronecker
    function} $\delta[i]$: 
    \begin{gather*}
        \delta[i] = \begin{cases}
            1 & \text{if } i = 0 \\
            0 & \text{otherwise}
        \end{cases}
    \end{gather*}
\end{itemize}

The impulse response $h$ completely characterises a \textit{linear
time-invariant} filter. Because any input signal can be form using impulses, if
one knows how the system may response to one impulse then you know how the system
will response to many impulses. 

\pagebreak

\textbf{Time-invariant system}: If a filter is a \textit{time-invariant system},
when the input is shifted by time step $k$, the output will also shift by $k$.
In the following example, note how the impulse response for $f[n]$ and $f[n-2]$
are the same but only shifted by $2$ in a LTI system.
\begin{figure}[h]
    \centering
    \includegraphics[width=8cm]{Impulse response shift 1.JPG} \\
    \includegraphics[width=8cm]{Impulse response shift 2.JPG}
\end{figure}

\textbf{Linear system}: If a filter is a \textit{linear system},
when the input is multiplied by $k$, the output will also by multiplied by $k$.
\begin{figure}[h]
    \centering
    \includegraphics[width=8cm]{Linear response.JPG} \\ 
    \includegraphics[width=8cm]{Linear response 2.JPG} 
\end{figure}

Similarly, if a filter is a linear system, when combining two input signals linearly,
their outputs will also be combined linearly.
\begin{figure}[h]
    \centering
    \includegraphics[width=8cm]{Linear combined.JPG}
\end{figure}

\section{Convolution and Linear Time-invariant System (LTI)}

For a linear time-invariant system, the impulse response $h$ completely
characterises how the LTI system works. The output $g$ can be described as the
\textit{convolution} between input $f$ and impulse response $h$:
$$
    g[n] = f[n] * h[n] = \sum_{m=-\infty}^\infty f[m]h[n-m]
$$

Convolutions are usually implemented by: 
\begin{enumerate}
    \item Flip the kernel 
    \item Multiply the signal with the flipped kernel 
    \item Sum over the support of the kernel 
\end{enumerate}
$$
    \sum_{m=-\infty}^\infty f[m]h[n-m] = \sum_{m=-\infty}^\infty f[n+m]h[-m]
$$

\subsection{Propoerties of convolution}
\begin{itemize}
    \item \textbf{Commutativity}: $f * h = h * f$
    \item \textbf{Associativity}: $f * (g * h) = (f * g) * h$
    \item \textbf{Distributivity}: $f * (g + h) = (f * g) + (f * h)$
    \item \textbf{Differentiation}: $\frac{d}{dx} (f * g) = \frac{df}{dx}*g = f*\frac{dg}{dx}$
\end{itemize}

\subsection{2D convolution}

2D convolution is defined mathematically as:
$$
    g[m,n] = f[m,n] * h[m,n] = \sum_{i=-\infty}^\infty \sum_{j=-\infty}^\infty f[i,j]h[m-i, n-j]
$$

2D convolutions are usually implemented by: 
\begin{enumerate}
    \item Flip the kernel both horizontally and vertically 
    \item Multiply the image patch centred at pixel $(m,n)$ with the flipped kernel
    \item Sum over the support of the kernel
\end{enumerate}
$$
    g[m,n] = f[m,n] * h[m,n] = \sum_{i=-\infty}^\infty \sum_{j=-\infty}^\infty f[m+i, n+j]h[-i,-j]
$$
\begin{figure}[h]
    \centering
    \includegraphics[width=8cm]{2D convolution.JPG}
\end{figure}

\subsection{Asspciativity and separable filters}

Note the associativity property of convolution:
$$
    f*(g*h) = (f*g)*h
$$
If a big filter can be separated as the convolution of two small filters, such
as $g*h$, then it is possible to first convolve $f$ with $g$ then with $h$.
$$
    f*filter_{big} = f*(g*h) = (f*g)*h
$$
\begin{figure}[h]
    \centering
    \includegraphics[width=8cm]{Associativity and separable.JPG}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Edge Detection I}

In computer vision, an edge refers to lines where image brightness changes
sharply and has discontinuities. Edges capture important properties of the world
and are important low-level features for analysing and understanding images.

To detect edges, we note that an image can be regarded as a function of pixel
position. As known from mathematics, derivatives characterises the
discontinuities of a function which can be used to help detect edges. For
example, consider the definition of a continuous function:
$$
    f'(x) = \lim_{h \rightarrow 0} \frac{f(x+h) - f(x)}{h}
$$
\begin{figure}[h]
    \centering
    \subfloat[\centering Continuous function]{{\includegraphics[width=5cm]{Edge derivative 1.JPG}}}%
    \qquad
    \subfloat[\centering Differentiated function showing detected discontinuity]{{\includegraphics[width=5cm]{Edge derivative 2.JPG}}}%
\end{figure}
 
For a discrete function, the finite difference can be calculated using:
\begin{itemize}
    \item Forward difference: $f'[x] = f[x+1] - f[x]$
    \item Backward difference: $f'[x] = f[x] - f[x-1]$
    \item Central difference: $f'[x] = \frac{f[x+1] - f[x-1]}{2}$
\end{itemize}

This finite difference can be performed using convolution with kernals such as:
\begin{itemize}
    \item $h=[1,-1,0]$
    \item $h=[0,1,-1]$
    \item $h=[1,0,-1]$
\end{itemize}

\section{Convolution}

As mentioned earlier, convolution is often implemented:
$$
    g[n] = f[n] * h[n] = \sum_{m=-\infty}^\infty f[n-m]h[m] = \sum_{m=-\infty}^\infty f[n+m]h[-m]
$$

The \textit{central difference}, without accounting for $\frac{1}{2}$, can be
defined as:
\begin{align*}
    f'[x] &= f[x+1] - f[x-1] \\
    &= f[x+1] *1 + f[x] * 0 + f[x-1] * (-1) \\
    &= f[x+1] * h[-1] + f[x] * h[0] + f[x-1] * h[1] 
\end{align*}

The convolution kernal is this defined as: $h = [1,0,-1]$. The process of
convolution with central difference $f'[x] = f[x] * h[x]$ can be seen as shown below: 
\begin{figure}[h]
    \centering
    \subfloat{{\includegraphics[width=8cm]{Edge convolution 1.JPG}}}%
    \qquad
    \subfloat{{\includegraphics[width=8cm]{Edge convolution 2.JPG}}}%
\end{figure}

Note the following:
\begin{itemize}
    \item The kernal $h[n]$ is flipped $h[-n]$ as mentioned earlier on how
    convolution implemented
    \item Large derivatives denote discontinuities 
\end{itemize}

The following methods only enable discontinuities to be detected in 1D. Similar
filters have to be designed to extend edge detection to a 2D image.

\section{Edge detection filters}

\subsection{Prewitt filter}
\begin{figure}[h]
    \centering
    \includegraphics[width=5cm]{Prewitt.JPG}
    \hspace{2cm}
    \includegraphics[width=6cm]{Prewitt separable.JPG}
\end{figure}

\subsection{Sobel filter}
\begin{figure}[h]
    \centering
    \includegraphics[width=5cm]{Sobel 1.JPG}
    \hspace{2cm}
    \includegraphics[width=6cm]{Sobel 2.JPG}
\end{figure}

\section{Image Gradient}

There are two outputs from Sobel filters that are combined to describe edges:
\begin{itemize}
    \item describing discontinuity along x-axis
    \item describing discontinuity along y-axis
\end{itemize}

The two outputs are combined to describe the two properties of edges: \textit{Magnitude} 
and \textit{Orientation} using the following process:
\begin{enumerate}
    \item Compute the derivatives along x-axis and y-axis:
    \begin{align*}
        g_x = f * h_x \\
        g_y = f * h_y
    \end{align*}
        
    \item Compute the magnitude of the gradient:
    $$
        g = \sqrt{g_x^2 + g_y^2}
    $$

    \item Compute the orientation or angle of the gradient:
    $$
        \theta = \arctan2(g_y, g_x)
    $$
\end{enumerate}

\section{Smoothing}

Derivatives are sensitive to noise thus using a smoothing kernel beforehand to
suppress the noise would result in a better result. This can be seen in the
Prewitt and Sobel filter which have a smoothing kernel built in. 

An alternative is to use the Gaussian kernel for smoothing before calculating
the derivatives. The following images show the ease of edge detection after
smoothing compared to no smoothing performed beforehand.
\begin{figure}[h]
    \centering
    \includegraphics[width=8cm]{Smoothing 1.JPG}
    \hspace{1cm}
    \includegraphics[width=8cm]{Smooth 2.JPG}
\end{figure}

\subsection{Derivative of Gaussian Filter}

\textit{Derivative of Gaussian filter} is an operator that performs Gaussian
smoothing before taking the derivative. 
\begin{gather*}
    g[x] = \frac{1}{\sqrt{2\pi} \sigma}e^{-\frac{x^2}{2\sigma^2}} \quad \text{Gaussian kernel} \\
    \frac{d}{dx}(f * h) \quad \text{Derivative of Gaussian filter}
\end{gather*}

Using the differentiation of convolution:
$$
    \frac{d}{dx}(f*h) = \frac{df}{dx} * h = f * \frac{dh}{dx} 
$$

The analytical form for the derivative of Gaussian kernel can be defined as:
\begin{figure}[h]
    \centering
    \includegraphics[width=5cm]{Gaussian derivative.JPG}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=9cm]{Derivative gaussian.JPG}
\end{figure}

\pagebreak

\begin{itemize}
    \item 1D Gaussian filter: $h[x] = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{x^2}{2\sigma^2}}$ 
    \item 2D Gaussian filter: $h[x, y] = \frac{1}{2\pi\sigma^2} e^{-\frac{x^2 + y^2}{2\sigma^2}}$ 
\end{itemize}

It is a separable filter and equivalent to convolution of two 1D Gaussian
filters. This means that 2D Gaussian smoothing can be accelerated using
separable filtering. 

The process of a Gaussian Derivative Filter is defined as:
\begin{enumerate}
    \item Smooth the input image with a 2D Gaussian filter 
    \item Take the derivative along x-axis and y-axis 
    \item Calculate the magnitude and orientation 
\end{enumerate}

\subsubsection{Parameter $\sigma$ in derivative of Gaussian}
Large $\sigma$ value suppresses noise and results smoother derivative. Different
$\sigma$ values finds edges at different scales. 
\begin{figure}[h]
    \centering
    \includegraphics[width=12cm]{Gaussian derivative sigma.JPG}
\end{figure}




\end{document}   